# RAG Pipeline for Answering Inference Queries

## Project Overview

This project aims to develop a Retrieval-Augmented Generation (RAG) pipeline that answers user queries using a given corpus of articles. The queries often require evidence from 2 to 4 documents and may need to reference metadata such as article titles, authors, publication dates, and categories. The pipeline's goal is to generate an answer based on available evidence, mimicking the format provided in the `train.json` dataset.

## Dataset Information

- **Corpus.json**: A collection of web-scraped articles. Each article entry contains:
  - `title`: Title of the article.
  - `author`: Author of the article.
  - `source`: Publication or website that published the article.
  - `published_at`: Date and time of publication.
  - `category`: General topic area (e.g., technology, business).
  - `url`: URL link to the original article.
  - `body`: The full text content of the article.

- **Train.json**: Contains 2,556 queries along with answers generated by a SOTA RAG pipeline. Each query involves:
  - `query`: The user's question.
  - `answer`: The inferred answer based on available evidence.
  - `question_type`: The nature of the question (e.g., "inference_query").
  - `evidence_list`: A list of 2 to 4 supporting documents with:
    - `title`, `author`, `url`, `source`, `category`, `published_at`, and `fact`: Specific information that supports the answer.

## Goal

The objective is to build a RAG-based solution that retrieves relevant documents and generates answers for user queries. The pipeline should:
- Retrieve 2 to 4 documents from `corpus.json` that provide relevant evidence for a query.
- Use document metadata (such as title, author, and publication date) as needed to craft a more precise response.
- Format the output in a structure similar to `train.json`, including `query`, `answer`, `question_type`, and `evidence_list`.

## Project Structure

- **data/**: Contains `corpus.json` and `train.json`.
- **src/**: The main source code for the RAG pipeline.
  - `retriever.py`: Retrieves the most relevant documents from the corpus.
  - `generator.py`: Generates an answer based on the retrieved documents.
  - `pipeline.py`: Integrates retrieval and generation to provide the final output.
  - `utils.py`: Utility functions for data preprocessing and handling.
- **models/**: Directory for pre-trained models and configurations.
- **notebooks/**: Jupyter notebooks for experimentation and analysis.
- **README.md**: Project documentation.
- **requirements.txt**: Python dependencies.

## Output Format

The output of the pipeline should match the following structure, as seen in `train.json`:

```json
{
  "query": "What is the impact of AI on the job market?",
  "answer": "AI is expected to create new job opportunities but also automate various roles, leading to a shift in skill requirements.",
  "question_type": "inference_query",
  "evidence_list": [
    {
      "title": "The Future of AI in Employment",
      "author": "John Doe",
      "url": "https://example.com/future-of-ai-employment",
      "source": "Example News",
      "category": "Technology",
      "published_at": "2023-05-01T08:00:00Z",
      "fact": "AI will create new jobs while automating routine tasks."
    },
    {
      "title": "Automation and Job Displacement",
      "author": "Jane Smith",
      "url": "https://example.com/automation-job-displacement",
      "source": "Tech World",
      "category": "Business",
      "published_at": "2023-06-15T10:00:00Z",
      "fact": "Automation could lead to the displacement of certain job categories."
    }
  ]
}

